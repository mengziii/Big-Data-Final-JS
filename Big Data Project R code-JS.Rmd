---
title: "CIND 820 Big Data Final Project"
author: "Jossa Soto"
date: "13/02/2022"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---

```{r setup, message=FALSE}
# Load needed libraries
library(tidyverse)
library(ROSE)
library(tidytext)
library(glue)
library(stringr)
library(ggplot2)
library(patchwork)
library(gt)
library(tm)
library(knitr)
library(NLP)
library(wordcloud)
library(RColorBrewer)
```

## Dataset
```{r}
twitter_balanced <- read.csv("Twitter US Airline Sentiment.csv", header = T, na.strings = c("","NA"))

table(twitter_balanced$airline_sentiment)
```
Data is imbalanced so sampling will be done so that the subset is balanced. This will be done by undersampling the negative and neutral group. 

```{r}
twitter_balanced$airline_sentiment <- as.factor(twitter_balanced$airline_sentiment)

twitter_pos_ind <- which(twitter_balanced$airline_sentiment == "positive")
twitter_neg_ind <- which(twitter_balanced$airline_sentiment == "negative")
twitter_neut_ind <- which(twitter_balanced$airline_sentiment == "neutral") 

set.seed(2407)
nsample <- 2500
pick_neg <- sample(twitter_neg_ind, nsample)
pick_neut <- sample(twitter_neut_ind, nsample)

twitter_balanced <- twitter_balanced[c(pick_neg, pick_neut, twitter_pos_ind),] 

table(twitter_balanced$airline_sentiment)
```

```{r}
# Declare each variable from df for easy use
tweet_id <- twitter_balanced$tweet_id
airline_sentiment <- twitter_balanced$airline_sentiment
airline_sentiment_confidence <- twitter_balanced$airline_sentiment_confidence
negativereason <- twitter_balanced$negativereason
negativereason_confidence <- twitter_balanced$negativereason_confidence
airline <- twitter_balanced$airline
airline_sentiment_gold <- twitter_balanced$airline_sentiment_gold
name <- twitter_balanced$name
negativereason_gold <- twitter_balanced$negativereason_gold
retweet_count <- twitter_balanced$retweet_count
tweet_text <- twitter_balanced$text
tweet_coord <- twitter_balanced$tweet_coord
tweet_created <- twitter_balanced$tweet_created
tweet_location <- twitter_balanced$tweet_location
user_TZ <- twitter_balanced$user_timezone

# Display first 6 rows of data
head(twitter_balanced)
```

## Data Exploration

```{r}
str(twitter_balanced)

# Change airline_sentiment to factors
twitter_balanced$airline <- as.factor(twitter_balanced$airline)
twitter_balanced$airline_sentiment <- as.factor(twitter_balanced$airline_sentiment)
twitter_balanced$airline_sentiment_gold <- as.factor(twitter_balanced$airline_sentiment_gold)
twitter_balanced$negativereason <- as.factor(twitter_balanced$negativereason)
twitter_balanced$negativereason_gold <- as.factor(twitter_balanced$negativereason_gold)

# Change tweet_created from char to date
twitter_balanced$tweet_created <- as.Date(twitter_balanced$tweet_created)

# Rerun str and summary to display updated variables
str(twitter_balanced)

# Display summary of the twitter_balanced data frame
summary(twitter_balanced)

# Standard deviation for numerical variables
sd(airline_sentiment_confidence); sd(!is.na(negativereason_confidence)); sd(retweet_count)
```

```{r}
# Check for and count any NAs
# Unlist to convert the list output to a vector for easier readability
unlist(lapply(lapply(twitter_balanced, is.na), sum)) 
```
```{r warning = FALSE}
# Check for any outliers in the numerical variables

# Airline sentiment confidence
ASC_box <- twitter_balanced %>% ggplot() + 
  geom_boxplot(aes(y = airline_sentiment_confidence), outlier.colour="black", outlier.shape=16, outlier.size=2, notch=FALSE) 

# Negative reason confidence
NRC_box <- twitter_balanced %>% 
  ggplot() + 
  geom_boxplot(aes(y = negativereason_confidence), outlier.colour="black", outlier.shape=16, outlier.size=2, notch=FALSE) 

# Retweet count
RC_box <- twitter_balanced %>% 
  ggplot() + 
  geom_boxplot(aes(y = retweet_count), outlier.colour="black", outlier.shape=16, outlier.size=2, notch=FALSE)

outlier_fig <- ASC_box + NRC_box + RC_box + plot_layout(ncol = 3)
outlier_fig
```

```{r}
# Frequency of the factor and discrete variables
table(airline)
table(airline_sentiment)
table(negativereason)
table(negativereason_gold)
table(airline_sentiment_gold)
table(retweet_count)
```

```{r}
# Airline
airline_bar <- twitter_balanced %>% ggplot() + 
  geom_bar(aes(x = airline, fill = airline)) +
  theme(axis.text.x = element_blank()) +
  ggtitle("Airline") + labs(y = "Count", x = "Airline", fill = "Airline")

# Airline sentiment
AS_bar <- twitter_balanced %>% ggplot() + 
  geom_bar(aes(x = airline_sentiment, fill = airline_sentiment)) +
  theme(axis.text.x = element_blank()) +
  ggtitle("Airline Sentiment (Class)") + labs(y = "Count", x = "Airline Sentiment (Class)", fill = "Airline Sentiment (Class)")

# Airline sentiment
ASG_bar <- twitter_balanced %>% ggplot() + 
  geom_bar(aes(x = airline_sentiment_gold, fill = airline_sentiment_gold)) +
  theme(axis.text.x = element_blank()) +
  ggtitle("Airline Sentiment - Gold") + labs(y = "Count", x = "Airline Sentiment - Gold", fill = "Airline Sentiment - Gold")

# Negative reason
NR_bar <- twitter_balanced %>% ggplot() + 
  geom_bar(aes(x = negativereason, fill = negativereason)) +
  theme(axis.text.x = element_blank()) +
  ggtitle("Negative Reason") + labs(y = "Count", x = "Negative Reason", fill = "Negative Reason")

# Negative reason gold
NRG_bar <- twitter_balanced %>% ggplot() + 
  geom_bar(aes(x = negativereason_gold, fill = negativereason_gold)) +
  theme(axis.text.x = element_blank()) +
  ggtitle("Negative Reason - Gold") + labs(y = "Count", x = "Negative Reason - Gold", fill = "Negative Reason - Gold")

frequency_fig1 <- airline_bar + AS_bar + ASG_bar + NRG_bar + plot_layout(ncol = 2, nrow = 2)
frequency_fig1
NR_bar
```
## Train / Test Data

```{r }
smp_size <- floor(0.7 * nrow(twitter_balanced))

set.seed(924)
train_ind <- sample(seq_len(nrow(twitter_balanced)), size = smp_size)

tweet_train <- twitter_balanced[train_ind, ]
tweet_test <- twitter_balanced[-train_ind, ]
```

## Preprocessing

```{r, warning = FALSE}
tweets_text <- twitter_balanced$text
#tweets_text <- boost_tokenizer(twitter_balanced$text)
str(tweets_text)

# Create a corpus
tweets_source <- VectorSource(tweets_text)

tweets_corpus <- Corpus(tweets_source)

tweets_corpus <- tm_map(tweets_corpus, function(x) iconv(enc2utf8(x), sub = "byte"))
```

```{r, warning = FALSE}
tweets_corpus <- tm_map(tweets_corpus, content_transformer(tolower)) # Convert all text to lowercase

tweets_corpus <- tm_map(tweets_corpus, removePunctuation) # Remove all punctuation

tweets_corpus <- tm_map(tweets_corpus, removeNumbers) # Remove numbers

# Custom function to remove extra elements/characters from the text
textprocessing <- function(x){
  gsub("http[[:alnum:]]*",'', x)
  gsub('http\\S+\\s*', '', x) # Remove URLs
  gsub('\\b+RT', '', x) # Remove RT
  gsub('#\\S+', '', x) # Remove Hashtags
  gsub('@\\S+', '', x) # Remove Mentions
  gsub('[[:cntrl:]]', '', x) # Remove Controls and special characters
  gsub("\\d", '', x) # Remove Controls and special characters
  gsub('[[:punct:]]', '', x) # Remove Punctuations
  gsub("^[[:space:]]*","",x) # Remove leading whitespaces
  gsub("[[:space:]]*$","",x) # Remove trailing whitespaces
  gsub(' +',' ',x) # Remove extra whitespaces 
}

tweets_corpus <- tm_map(tweets_corpus, textprocessing)

tweets_corpus <- tm_map(tweets_corpus, stripWhitespace) # Remove whitespace

# Create custom list of stop words
stop_words <- c(stopwords("english"), "rt","southwestair", "americanair", "delta", "united", "usairway", "virginamerica", "jetblue", "amp")

tweets_corpus <- tm_map(tweets_corpus, removeWords, stop_words) # Remove stop words

# Create a copy of tweets pre-stemming
tweets_corpus_copy <- tweets_corpus

tweets_corpus <- tm_map(tweets_corpus, stemDocument) # Stem the text

# Display a row of the cleaned corpus
tweets_corpus[[148]][1]
```
### Term-Document: TF-IDF Weights

Create a term-document matrix with TF-IDF weights then reduce the matrix down so it only contains a certain \% of the terms. 

Weights are calculated using the following formula: 

$TF-IDF(t)=TF(t) \times IDF(t)$
\n $w_{i,j} = tf_{i,j} \times log\frac{N}{df_i}$

```{r, warning = FALSE}
tweets_tdm_tfidf <- TermDocumentMatrix(tweets_corpus, control = list(weighting = weightTfIdf))
tweets_tdm_tfidf

tweets_m_tfidf <- as.matrix(tweets_tdm_tfidf)
dim(tweets_m_tfidf)

# Display a subset of the matrix
tweets_m_tfidf[14:28, 10:15]

# Reduce the matrix by removing the low frequency terms
tweets_tdm_tfidf_sparse <- removeSparseTerms(tweets_tdm_tfidf, 0.99) 
tweets_tdm_tfidf_sparse

tweets_m_tfidf <- as.matrix(tweets_tdm_tfidf_sparse)
dim(tweets_m_tfidf)

# Display a subset of the matrix after the low frequency terms were removed
tweets_m_tfidf[14:28, 10:15]
```
```{r}
freq_tfidf <- rowSums(tweets_m_tfidf)

# # Plot of frequency terms appearing by summing the content of all terms
# plot(sort(freq_tfidf, decreasing = TRUE), col = "blue", main = "Word TF-IDF Frequencies", xlab = "TF-IDF-based rank", ylab = "TF-IDF")
```

```{r}
# Show most frequent terms and their frequencies
high_freq_tfidf <- tail(sort(freq_tfidf), n=15)
high_freq_tfidf_df <- as.data.frame(sort(high_freq_tfidf))
high_freq_tfidf_df$names <- rownames(high_freq_tfidf_df)

ggplot(high_freq_tfidf_df, aes(reorder(names,high_freq_tfidf), high_freq_tfidf)) +
  geom_bar(stat="identity", fill = "coral1") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies")
```

### Bag of Words

```{r}
tweets_tdm_bow <- TermDocumentMatrix(tweets_corpus)
tweets_tdm_bow

tweets_m_bow <- as.matrix(tweets_tdm_bow)
dim(tweets_m_bow)

# Display a subset of the matrix
tweets_m_bow[14:28, 10:15]

# Reduce the matrix by removing the low frequency terms
tweets_tdm_bow_sparse <- removeSparseTerms(tweets_tdm_bow, 0.99) 
tweets_tdm_bow_sparse

tweets_m_bow <- as.matrix(tweets_tdm_bow_sparse)
dim(tweets_m_bow)

# Display a subset of the matrix after the low frequency terms were removed
tweets_m_bow[14:28, 10:15]
```

```{r}
 freq_bow <- rowSums(tweets_m_bow)

# # Plot of frequency terms appearing by summing the content of all terms
# plot(sort(freq_bow, decreasing = TRUE), col = "blue", main = "Word Frequencies", xlab = "Bag of Words -based rank", ylab = "TF-IDF")
```

```{r}
# Show most frequent terms and their frequencies
high_freq_bow <- tail(sort(freq_bow), n=15)
high_freq_bow_df <- as.data.frame(sort(high_freq_bow))
high_freq_bow_df$names <- rownames(high_freq_bow_df)

ggplot(high_freq_bow_df, aes(reorder(names,high_freq_bow), high_freq_bow)) +
  geom_bar(stat="identity", fill = "darkseagreen") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies")
```

### N-grams

```{r}
Unigramtokenizer <- function(x) {
        unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)}

Bigramtokenizer <- function(x) {
        unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)}
```

#### Unigrams

```{r, warning = FALSE}
# Unigram TDM
tweets_tdm_uni <- TermDocumentMatrix(tweets_corpus,control = list(tokenize = Unigramtokenizer))

tweets_m_uni <- as.matrix(tweets_tdm_uni)
dim(tweets_m_uni)

# Display a subset of the matrix
tweets_m_uni[14:28, 10:15]

# Reduce the matrix by removing the low frequency terms
tweets_tdm_uni_sparse <- removeSparseTerms(tweets_tdm_uni, 0.99) 
tweets_tdm_uni_sparse

tweets_m_uni <- as.matrix(tweets_tdm_uni_sparse)
dim(tweets_m_uni)

# Display a subset of the matrix after the low frequency terms were removed
tweets_m_uni[14:28, 10:15]
```

```{r}
freq_uni <- rowSums(tweets_m_uni)

# Plot of frequency terms appearing by summing the content of all terms
# plot(sort(freq_uni, decreasing = TRUE), col = "blue", main = "Word Frequencies", xlab = "Unigram-based rank", ylab = "TF-IDF")
```

```{r}
# Show most frequent terms and their frequencies
high_freq_uni <- tail(sort(freq_uni), n=15)
high_freq_uni_df <- as.data.frame(sort(high_freq_uni))
high_freq_uni_df$names <- rownames(high_freq_uni_df)

ggplot(high_freq_uni_df, aes(reorder(names,high_freq_uni), high_freq_uni)) +
  geom_bar(stat="identity", fill = "cornflowerblue") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies")
```

#### Bigrams

```{r, warning = FALSE}

tweets_tdm_bin <- TermDocumentMatrix(tweets_corpus,control = list(tokenize = Bigramtokenizer))

tweets_m_bin <- as.matrix(tweets_tdm_bin)
dim(tweets_m_bin)

# Display a subset of the matrix
tweets_m_bin[14:28, 10:15]

# Reduce the matrix by removing the low frequency terms
tweets_tdm_bin_sparse <- removeSparseTerms(tweets_tdm_bin, 0.99) 
tweets_tdm_bin_sparse

tweets_m_bin <- as.matrix(tweets_tdm_bin_sparse)
dim(tweets_m_bin)

# Display a subset of the matrix after the low frequency terms were removed
tweets_m_bin[14:28, 10:15]
```

```{r}
freq_bin <- rowSums(tweets_m_bin)

# Plot of frequency terms appearing by summing the content of all terms
# plot(sort(freq_uni, decreasing = TRUE), col = "blue", main = "Word Frequencies", xlab = "Bigram-based rank", ylab = "TF-IDF")
```

```{r, warning = FALSE}
# Show most frequent terms and their frequencies
high_freq_bin <- tail(sort(freq_bin), n=15)
high_freq_bin_df <- as.data.frame(sort(high_freq_bin))
high_freq_bin_df$names <- rownames(high_freq_bin_df)

ggplot(high_freq_bin_df, aes(reorder(names,high_freq_bin), high_freq_bin)) +
  geom_bar(stat="identity", fill = "darkorange") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies")
```